{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook settings\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%autosave 1\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting settings\n",
    "import matplotlib.pyplot as plt\n",
    "size=35\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (20,8),\n",
    "          'axes.labelsize': size,\n",
    "          'axes.titlesize': size,\n",
    "          'xtick.labelsize': size*0.75,\n",
    "          'ytick.labelsize': size*0.75,\n",
    "          'axes.titlepad': 25}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing files\n",
    "\n",
    "Every file under any folder in ./data is parsed and put into dictionaries that group videos of the same source.\n",
    "Videos of the same source are considered videos coming from the same Subject, same Session, same OD/OS, same (x, y),\n",
    "same type (Confocal, OA790, OA850).\n",
    "\n",
    "The parsing is case insensitive with the following rules:\n",
    "\n",
    "**unmarked videos**:\n",
    "must not contain 'mask' or '_marked'\n",
    "\n",
    "**marked videos**:\n",
    "Must end with '_marked.\\<<file_extension\\>>'\n",
    "\n",
    "**standard deviation images**:\n",
    "Must end with\n",
    "'_std.\\<<file_extension\\>>'\n",
    "\n",
    "**vessel mask images**: \n",
    "Must end with\n",
    "'_vessel_mask.\\<<file_extension\\>>'\n",
    "\n",
    "**channel type**:\n",
    "must contain one of 'OA790', 'OA850', 'Confocal' (case insensitive)\n",
    "\n",
    "### Example to using VideoSession objects and the get_video_session() function\n",
    "(useful for training and training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from os.path import basename\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True,\n",
    "                                    should_be_registered=True)\n",
    "for session in video_sessions:\n",
    "        assert session.has_marked_video\n",
    "        assert session.is_registered\n",
    "        assert session.has_marked_cells\n",
    "        print('-----------------------')\n",
    "        print('Video file:', basename(session.video_file))\n",
    "        print('Does video have a corresponding marked video?:', session.has_marked_video)\n",
    "        print('Subject number:', session.subject_number)\n",
    "        print('Session number:', session.session_number)\n",
    "        print('Marked Video OA790:', basename(session.marked_video_oa790_file))\n",
    "        print('Std dev image confocal:', basename(session.std_image_confocal_file))\n",
    "        print('Std dev image OA850:', basename(session.std_image_oa850_file))\n",
    "        print('Vessel mask OA850:', basename(session.vessel_mask_oa850_file))\n",
    "        print('Vessel mask confocal:', basename(session.vessel_mask_confocal_file))\n",
    "        print('Cell position csv files:', *[basename(f) for f in session.cell_position_csv_files], sep='\\n')\n",
    "        print()\n",
    "        \n",
    "print('Number of video sessions ', len(video_sessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading frames from videos - (and cell positions for each frame)\n",
    "\n",
    "You can get access to the frames of the video session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import no_ticks\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True,\n",
    "                                    should_be_registered=True)\n",
    "_, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "no_ticks(axes)\n",
    "\n",
    "axes[0].imshow(session.frames_oa790[0], cmap='gray')\n",
    "axes[0].set_title(f\"First frame of {basename(session.video_oa790_file)}\", fontsize=10)\n",
    "axes[0].scatter(session.cell_positions[0][:, 0], session.cell_positions[0][:, 1], label='cell positions', s=10)\n",
    "axes[0].legend()\n",
    "    \n",
    "axes[1].imshow(session.marked_frames_oa790[0], cmap='gray')\n",
    "axes[1].set_title(f\"First marked frame of {basename(session.marked_video_oa790_file)}\", fontsize=10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract cell and no cell patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SessionPatchExtractor (Object oriented way) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(patch_extractor.marked_cell_patches_oa790[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True)\n",
    "vs = video_sessions[5]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(vs, patch_size=21, n_negatives_per_positive=1)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.non_cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_non_cell_patches_oa790[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(vs, patch_size=21, temporal_width=1, n_negatives_per_positive=7)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.temporal_cell_patches_oa790[:10], title='Temporal cell patches temporal width 1')\n",
    "plot_images_as_grid(patch_extractor.temporal_marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.temporal_non_cell_patches_oa790[:10], title='Temporal non cell patches temporal width 1')\n",
    "plot_images_as_grid(patch_extractor.temporal_marked_non_cell_patches_oa790[:10])\n",
    "\n",
    "# A higher temporal width will give patches with more channells\n",
    "patch_extractor.temporal_width = 1\n",
    "print(f'Temporal patches shape with temporal width = 1: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 4\n",
    "print(f'Temporal patches shape with temporal width = 4: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 5\n",
    "print(f'Temporal patches shape with temporal width = 5: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 6\n",
    "print(f'Temporal patches shape with temporal width = 6: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "print(f'As temporal window becomes bigger notice that there are less patches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed channel patches\n",
    " \n",
    "Mixed channel patches give patches with 3 channels, the first channel is confocal video patch, second channel is from the oa780 channel,\n",
    "third channel is from the oa850 channel.\n",
    "\n",
    "The confocal video and the oa790 channel have the capillaries at the same position. The oa850 video has a vertical displacement, the video is registered before extracting the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(vs, patch_size=21)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_cell_patches[:10])\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_marked_cell_patches[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_non_cell_patches[:10])\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_marked_non_cell_patches[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor.visualize_mixed_channel_patch_extraction(frame_idx=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset convenience functions\n",
    "\n",
    "Extract all patches (marked and unmarkded) for all the video session given.\n",
    "\n",
    "If no video sessions given then all the video sessions automatically created from the videos with cell position csv in the data folder are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=reg_video_sessions,                                                                  \n",
    "    n_negatives_per_positive=1,                                                                                                \n",
    "    v=True,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10])\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    temporal_width=1,\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title='Temporal width 1')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed channel patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    mixed_channel_patches=True,\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title='Mixed channel patches')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageprosessing import hist_match_images\n",
    "\n",
    "def get_highest_contrast_frame(video_sessions):\n",
    "    max_diff = 0\n",
    "    max_diff_idx = 0\n",
    "    for i, vs in enumerate(video_sessions):\n",
    "        vs.mask_frames_oa790 = crop_mask(vs.mask_frames_oa790, 15)\n",
    "        diff = vs.masked_frames_oa790[0].max() - vs.masked_frames_oa790[0].min()\n",
    "        if diff > max_diff:\n",
    "            max_diff_idx = i\n",
    "            max_diff = diff\n",
    "            \n",
    "    highest_contrast_frame = video_sessions[max_diff_idx].masked_frames_oa790[0]\n",
    "    highest_contrast_frame = highest_contrast_frame.filled(highest_contrast_frame.mean())\n",
    "    \n",
    "    return highest_contrast_frame\n",
    "\n",
    "template_frame = get_highest_contrast_frame(reg_video_sessions)\n",
    "\n",
    "hist_matched_cell_images = hist_match_images(cell_images, template_frame)\n",
    "hist_matched_non_cell_images = hist_match_images(non_cell_images, template_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from cnnlearning import CNN, train, TrainingTracker \n",
    "\n",
    "standardize_dataset = True\n",
    "trainset, validset = create_dataset_from_cell_and_no_cell_images(hist_matched_cell_images, \n",
    "                                                                 hist_matched_non_cell_images,\n",
    "                                                                 validset_ratio=0.2,\n",
    "                                                                 standardize=True)\n",
    "\n",
    "model = CNN(dataset_sample=trainset, output_classes=2).to('cuda')\n",
    "train_params = collections.OrderedDict(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=.001, weight_decay=0.01),\n",
    "    batch_size=256,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    early_stop_patience=40,\n",
    "    learning_rate_scheduler_patience=20,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    # valid_untrunsformed_normals = valid_untrunsformed_normals,\n",
    "    evaluation_epochs=5,\n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    ")\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classificationutils import classify_labeled_dataset, classify_images\n",
    "\n",
    "model = results.recorded_model\n",
    "model.eval()\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, model)\n",
    "positive_accuracy = classify_images(cell_images, model, standardize_dataset=standardize_dataset).sum().item() / len(cell_images)\n",
    "negative_accuracy = (1 - classify_images(non_cell_images, model, standardize_dataset=standardize_dataset)).sum().item() / len(non_cell_images)\n",
    "\n",
    "print()\n",
    "print(f'Model trained on {len(cell_images)} cell patches and {len(non_cell_images)} non cell patches.')\n",
    "print()\n",
    "print('Brief evaluation - best validation accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')\n",
    "\n",
    "train_model = results.recorded_train_model\n",
    "train_model.eval()\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, train_model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, train_model)\n",
    "positive_accuracy = classify_images(cell_images, train_model, standardize_dataset=standardize_dataset).sum().item() / len(cell_images)\n",
    "negative_accuracy = (1 - classify_images(non_cell_images, train_model, standardize_dataset=standardize_dataset)).sum().item() / len(non_cell_images)\n",
    "\n",
    "print()\n",
    "print('Brief evaluation - best training accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_train_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
